{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for missing libraries and installing them if necessary...\n",
      "Installing scikit-learn...\n",
      "All required libraries are installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrad/.local/lib/python3.9/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Churn prediction model with optimized data processing\n",
    "# Author: Hose I. Rad\n",
    "# Date: Oct. 19th 2024\n",
    "\n",
    "\"\"\"\n",
    "This code builds a churn prediction model for an eCommerce platform using user event data.\n",
    "Due to the large size of the dataset, we tried to optimize data processing steps to handle it efficiently.\n",
    "\n",
    "Key Features:\n",
    "- Processes data files in chunks to manage memory usage.\n",
    "- Utilizes multiprocessing to speed up file processing.\n",
    "- Extracts meaningful features for churn prediction.\n",
    "- Implements feature engineering techniques.\n",
    "- Addresses class imbalance using SMOTE.\n",
    "- Performs hyperparameter tuning for model optimization.\n",
    "- Trains an XGBoost classifier for improved accuracy.\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Function to install missing libraries\n",
    "def install_library(package):\n",
    "    \"\"\"\n",
    "    Installs a library using pip if it's not already installed.\n",
    "\n",
    "    Args:\n",
    "        package (str): The name of the package to install.\n",
    "    \"\"\"\n",
    "    if package == 'sklearn':\n",
    "        package = 'scikit-learn'\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# List of required libraries with optional aliases\n",
    "required_libraries = {\n",
    "    'pandas': 'pd',\n",
    "    'numpy': 'np',\n",
    "    'matplotlib': 'plt',\n",
    "    'seaborn': 'sns',\n",
    "    'scikit-learn': None,\n",
    "    'xgboost': None,\n",
    "    'os': None,\n",
    "    'glob': None,\n",
    "    'imblearn': None,\n",
    "    'joblib': None,\n",
    "    'multiprocessing': None,\n",
    "    'warnings': None\n",
    "}\n",
    "\n",
    "# Check and install any missing libraries\n",
    "print(\"Checking for missing libraries and installing them if necessary...\")\n",
    "for lib, alias in required_libraries.items():\n",
    "    try:\n",
    "        if alias:\n",
    "            globals()[alias] = __import__(lib)\n",
    "        else:\n",
    "            __import__(lib)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {lib}...\")\n",
    "        install_library(lib)\n",
    "print(\"All required libraries are installed.\")\n",
    "\n",
    "# Now import the libraries after installation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.pipeline import Pipeline  # Use Pipeline from imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import reduce\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the plot style for seaborn\n",
    "sns.set(style='whitegrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single file and extract features\n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Processes a single CSV file to extract features needed for churn prediction.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the extracted features.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "    # Determine if the file is compressed\n",
    "    compression = 'gzip' if file_path.endswith('.gz') else None\n",
    "\n",
    "    # Initialize a list to store feature DataFrames from each chunk\n",
    "    feature_chunks = []\n",
    "\n",
    "    # Read the file in chunks to handle large files\n",
    "    for chunk in pd.read_csv(\n",
    "        file_path,\n",
    "        compression=compression,\n",
    "        parse_dates=['event_time'],\n",
    "        low_memory=False,\n",
    "        chunksize=5_000_000,  # Adjust this number based on your system's memory\n",
    "        dtype={\n",
    "            'event_type': 'category',\n",
    "            'product_id': 'int32',\n",
    "            'category_id': 'float32',\n",
    "            'category_code': 'object',\n",
    "            'brand': 'object',\n",
    "            'price': 'float32',\n",
    "            'user_id': 'int32',\n",
    "            'user_session': 'object'\n",
    "        }\n",
    "    ):\n",
    "        # Handle missing values in 'category_code' and 'brand'\n",
    "        chunk['category_code'] = chunk['category_code'].fillna('unknown')\n",
    "        chunk['brand'] = chunk['brand'].fillna('unknown')\n",
    "\n",
    "        # Convert event time to datetime and extract additional time features\n",
    "        chunk['event_time'] = pd.to_datetime(chunk['event_time']).dt.tz_localize(None)\n",
    "        chunk['event_date'] = chunk['event_time'].dt.date\n",
    "        chunk['month'] = chunk['event_time'].dt.to_period('M')\n",
    "\n",
    "        # Group the data to compute features for each user in each month using the new syntax\n",
    "        features = chunk.groupby(['user_id', 'month']).agg(\n",
    "            total_events=('event_type', 'count'),\n",
    "            unique_event_types=('event_type', 'nunique'),\n",
    "            num_views=('event_type', lambda x: (x == 'view').sum()),\n",
    "            num_carts=('event_type', lambda x: (x == 'cart').sum()),\n",
    "            num_purchases=('event_type', lambda x: (x == 'purchase').sum()),\n",
    "            num_remove_from_cart=('event_type', lambda x: (x == 'remove_from_cart').sum()),\n",
    "            num_unique_products=('product_id', 'nunique'),\n",
    "            num_unique_categories=('category_code', 'nunique'),\n",
    "            avg_price=('price', 'mean'),\n",
    "            max_price=('price', 'max'),\n",
    "            min_price=('price', 'min'),\n",
    "            num_sessions=('user_session', 'nunique'),\n",
    "            active_days=('event_date', 'nunique'),\n",
    "            first_event_time=('event_time', 'min'),\n",
    "            last_event_time=('event_time', 'max')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Append the features from this chunk to our list\n",
    "        feature_chunks.append(features)\n",
    "\n",
    "    # If we have collected any features, proceed to aggregate them\n",
    "    if feature_chunks:\n",
    "        # Concatenate all the feature DataFrames from the chunks\n",
    "        df_features = pd.concat(feature_chunks, ignore_index=True)\n",
    "        del feature_chunks  # Free up memory\n",
    "\n",
    "        # Aggregate features across all chunks for the same user and month\n",
    "        df_features = df_features.groupby(['user_id', 'month']).agg({\n",
    "            'total_events': 'sum',\n",
    "            'unique_event_types': 'sum',\n",
    "            'num_views': 'sum',\n",
    "            'num_carts': 'sum',\n",
    "            'num_purchases': 'sum',\n",
    "            'num_remove_from_cart': 'sum',\n",
    "            'num_unique_products': 'sum',\n",
    "            'num_unique_categories': 'sum',\n",
    "            'avg_price': 'mean',\n",
    "            'max_price': 'max',\n",
    "            'min_price': 'min',\n",
    "            'num_sessions': 'sum',\n",
    "            'active_days': 'sum',\n",
    "            'first_event_time': 'min',\n",
    "            'last_event_time': 'max'\n",
    "        }).reset_index()\n",
    "\n",
    "        # Calculate additional ratios that might be useful\n",
    "        df_features['view_to_purchase_ratio'] = df_features['num_views'] / df_features['num_purchases']\n",
    "        df_features['cart_to_purchase_ratio'] = df_features['num_carts'] / df_features['num_purchases']\n",
    "        df_features.replace([np.inf, np.nan], 0, inplace=True)\n",
    "\n",
    "        # Calculate session duration\n",
    "        df_features['session_duration'] = (df_features['last_event_time'] - df_features['first_event_time']).dt.total_seconds()\n",
    "        df_features['session_duration'].fillna(0, inplace=True)\n",
    "\n",
    "        # Calculate recency\n",
    "        max_date = df_features['last_event_time'].max()\n",
    "        df_features['recency'] = (max_date - df_features['last_event_time']).dt.days\n",
    "\n",
    "        # Drop unneeded columns\n",
    "        df_features.drop(['first_event_time', 'last_event_time'], axis=1, inplace=True)\n",
    "\n",
    "        return df_features\n",
    "    else:\n",
    "        # If no features were extracted, return an empty DataFrame\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to process all files and prepare the dataset\n",
    "def process_data(data_path):\n",
    "    \"\"\"\n",
    "    Processes all data files in the specified directory to extract features.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The directory containing the data files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing features from all files.\n",
    "    \"\"\"\n",
    "    # Get a sorted list of all CSV files in the data directory\n",
    "    files = sorted(glob(os.path.join(data_path, '*.csv*')))\n",
    "\n",
    "    # Use multiprocessing to process files in parallel\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        features_list = pool.map(process_file, files)\n",
    "\n",
    "    # Filter out any empty DataFrames\n",
    "    features_list = [df for df in features_list if not df.empty]\n",
    "\n",
    "    # Concatenate features from all files\n",
    "    if features_list:\n",
    "        features_df = pd.concat(features_list, ignore_index=True)\n",
    "        # Aggregate features across all files\n",
    "        features_df = features_df.groupby(['user_id', 'month']).sum().reset_index()\n",
    "        return features_df\n",
    "    else:\n",
    "        print(\"No data processed.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to define churn based on user activity\n",
    "def define_churn(features_df):\n",
    "    \"\"\"\n",
    "    Defines churn for each user based on their activity in the subsequent months.\n",
    "\n",
    "    Args:\n",
    "        features_df (pd.DataFrame): The DataFrame containing user features.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with churn labels for each user.\n",
    "    \"\"\"\n",
    "    # Get a sorted list of all months in the data\n",
    "    months = sorted(features_df['month'].unique())\n",
    "\n",
    "    churn_data = []\n",
    "\n",
    "    # For each month, determine if users have churned\n",
    "    for idx, month in enumerate(months[:-2]):\n",
    "        current_month = month\n",
    "        next_months = months[idx + 1: idx + 3]\n",
    "\n",
    "        users_current = set(features_df[features_df['month'] == current_month]['user_id'])\n",
    "        users_next = set(features_df[features_df['month'].isin(next_months)]['user_id'])\n",
    "\n",
    "        for user in users_current:\n",
    "            # A user is considered churned if they are not active in the next two months\n",
    "            churn = 1 if user not in users_next else 0\n",
    "            churn_data.append({'user_id': user, 'month': current_month, 'churn': churn})\n",
    "\n",
    "    churn_df = pd.DataFrame(churn_data)\n",
    "    return churn_df\n",
    "\n",
    "# Function to prepare the dataset for modeling\n",
    "def prepare_data(features_df, churn_df):\n",
    "    \"\"\"\n",
    "    Merges the features and churn labels into a single DataFrame for modeling.\n",
    "\n",
    "    Args:\n",
    "        features_df (pd.DataFrame): The DataFrame containing user features.\n",
    "        churn_df (pd.DataFrame): The DataFrame containing churn labels.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The merged DataFrame ready for modeling.\n",
    "    \"\"\"\n",
    "    # Merge features with churn labels\n",
    "    data = pd.merge(features_df, churn_df, on=['user_id', 'month'], how='inner')\n",
    "    # Fill any missing values with zero\n",
    "    data.fillna(0, inplace=True)\n",
    "    return data\n",
    "\n",
    "# Function to train the churn prediction model\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost classifier to predict churn.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): The training features.\n",
    "        y_train (pd.Series): The training labels.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: The trained model pipeline.\n",
    "    \"\"\"\n",
    "    # Handle class imbalance using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "\n",
    "    # Define the classifier\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Create a pipeline with scaling, SMOTE, and the classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('smote', smote),\n",
    "        ('classifier', xgb)\n",
    "    ])\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [3, 5, 7],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__subsample': [0.8, 1.0],\n",
    "        'classifier__colsample_bytree': [0.8, 1.0],\n",
    "    }\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=10,\n",
    "        scoring='roc_auc',\n",
    "        cv=3,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters found: {search.best_params_}\")\n",
    "    return search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the churn prediction model\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost classifier to predict churn.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): The training features.\n",
    "        y_train (pd.Series): The training labels.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: The trained model pipeline.\n",
    "    \"\"\"\n",
    "    # Handle class imbalance using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "\n",
    "    # Define the classifier\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Create a pipeline with scaling, SMOTE, and the classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('smote', smote),\n",
    "        ('classifier', xgb)\n",
    "    ])\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [3, 5, 7],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__subsample': [0.8, 1.0],\n",
    "        'classifier__colsample_bytree': [0.8, 1.0],\n",
    "    }\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=10,\n",
    "        scoring='roc_auc',\n",
    "        cv=3,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters found: {search.best_params_}\")\n",
    "    return search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution Block\n",
    "\n",
    "print(\"Starting churn prediction workflow...\")\n",
    "# Set the path to your data directory\n",
    "data_path = '/sas_new/sasuser/CMJP/hrd/churn_predict'  # Replace with your actual data path\n",
    "print(f\"Data path set to: {data_path}\")\n",
    "\n",
    "# Check if features DataFrame exists\n",
    "features_file = 'features_df.pkl'\n",
    "if os.path.exists(features_file):\n",
    "    print(f\"Features DataFrame found at {features_file}. Loading it...\")\n",
    "    features_df = pd.read_pickle(features_file)\n",
    "else:\n",
    "    # Process the data files and extract features\n",
    "    print(\"Processing data files...\")\n",
    "    features_df = process_data(data_path)\n",
    "    # Save the features DataFrame to a file\n",
    "    features_df.to_pickle(features_file)\n",
    "    print(f\"Features DataFrame saved to {features_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(features_df, churn_df):\n",
    "    \"\"\"\n",
    "    Merges the features and churn labels into a single DataFrame for modeling.\n",
    "\n",
    "    Args:\n",
    "        features_df (pd.DataFrame): The DataFrame containing user features.\n",
    "        churn_df (pd.DataFrame): The DataFrame containing churn labels.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The merged DataFrame ready for modeling.\n",
    "    \"\"\"\n",
    "    # Merge features with churn labels\n",
    "    data = pd.merge(features_df, churn_df, on=['user_id', 'month'], how='inner')\n",
    "\n",
    "    # Exclude 'month' from fillna by identifying non-Period columns manually\n",
    "    # Assuming 'month' is the only Period dtype column\n",
    "    non_period_cols = [col for col in data.columns if col != 'month']\n",
    "\n",
    "    # Fill missing values only for non-Period columns\n",
    "    data[non_period_cols] = data[non_period_cols].fillna(0)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features DataFrame:\n",
      "    user_id    month  total_events  unique_event_types  num_views  num_carts  \\\n",
      "0  10300217  2019-11             1                   1          1          0   \n",
      "1  12511517  2020-02             1                   1          1          0   \n",
      "2  12511517  2020-03             1                   1          1          0   \n",
      "3  22165363  2020-01             2                   1          2          0   \n",
      "4  22165363  2020-02            10                   4         10          0   \n",
      "\n",
      "   num_purchases  num_remove_from_cart  num_unique_products  \\\n",
      "0              0                     0                    1   \n",
      "1              0                     0                    1   \n",
      "2              0                     0                    1   \n",
      "3              0                     0                    1   \n",
      "4              0                     0                    8   \n",
      "\n",
      "   num_unique_categories   avg_price   max_price   min_price  num_sessions  \\\n",
      "0                      1   40.540001   40.540001   40.540001             1   \n",
      "1                      1  100.389999  100.389999  100.389999             1   \n",
      "2                      1   17.629999   17.629999   17.629999             1   \n",
      "3                      1  253.550003  253.550003  253.550003             2   \n",
      "4                      6  144.794586  386.109985   15.830000             9   \n",
      "\n",
      "   active_days  view_to_purchase_ratio  cart_to_purchase_ratio  \\\n",
      "0            1                     0.0                     0.0   \n",
      "1            1                     0.0                     0.0   \n",
      "2            1                     0.0                     0.0   \n",
      "3            2                     0.0                     0.0   \n",
      "4            5                     0.0                     0.0   \n",
      "\n",
      "   session_duration  recency  \n",
      "0               0.0       24  \n",
      "1               0.0        5  \n",
      "2               0.0       23  \n",
      "3           91568.0        0  \n",
      "4         1391651.0        1  \n",
      "Churn DataFrame found at churn_df.pkl. Loading it...\n",
      "Preparing final dataset for making the classification model...\n",
      "\n",
      "Merged DataFrame (Features + Churn Labels):\n",
      "    user_id    month  total_events  unique_event_types  num_views  num_carts  \\\n",
      "0  10300217  2019-11             1                   1          1          0   \n",
      "1  12511517  2020-02             1                   1          1          0   \n",
      "2  22165363  2020-01             2                   1          2          0   \n",
      "3  22165363  2020-02            10                   4         10          0   \n",
      "4  29515875  2019-11            11                   2         11          0   \n",
      "\n",
      "   num_purchases  num_remove_from_cart  num_unique_products  \\\n",
      "0              0                     0                    1   \n",
      "1              0                     0                    1   \n",
      "2              0                     0                    1   \n",
      "3              0                     0                    8   \n",
      "4              0                     0                    6   \n",
      "\n",
      "   num_unique_categories   avg_price   max_price   min_price  num_sessions  \\\n",
      "0                      1   40.540001   40.540001   40.540001             1   \n",
      "1                      1  100.389999  100.389999  100.389999             1   \n",
      "2                      1  253.550003  253.550003  253.550003             2   \n",
      "3                      6  144.794586  386.109985   15.830000             9   \n",
      "4                      3  383.001007  514.809998  100.360001             7   \n",
      "\n",
      "   active_days  view_to_purchase_ratio  cart_to_purchase_ratio  \\\n",
      "0            1                     0.0                     0.0   \n",
      "1            1                     0.0                     0.0   \n",
      "2            2                     0.0                     0.0   \n",
      "3            5                     0.0                     0.0   \n",
      "4            3                     0.0                     0.0   \n",
      "\n",
      "   session_duration  recency  churn  \n",
      "0               0.0       24      1  \n",
      "1               0.0        5      0  \n",
      "2           91568.0        0      0  \n",
      "3         1391651.0        1      0  \n",
      "4          913862.0       10      0  \n",
      "Prepared data saved to prepared_data.pkl.\n",
      "Prepared data found at prepared_data.pkl. Loading it...\n",
      "Splitting data into training and testing sets...\n",
      "Separating features and target variable...\n",
      "Training the model...\n"
     ]
    }
   ],
   "source": [
    "# Check if any data was processed\n",
    "if features_df.empty:\n",
    "    print(\"No data available after processing. Exiting.\")\n",
    "else:\n",
    "    print(\"\\nFeatures DataFrame:\")\n",
    "    print(features_df.head())\n",
    "\n",
    "# Check if churn labels DataFrame exists\n",
    "churn_file = 'churn_df.pkl'\n",
    "if os.path.exists(churn_file):\n",
    "    print(f\"Churn DataFrame found at {churn_file}. Loading it...\")\n",
    "    churn_df = pd.read_pickle(churn_file)\n",
    "else:\n",
    "    print(\"Defining churn labels...\")\n",
    "    churn_df = define_churn(features_df)\n",
    "    # Save churn labels to file\n",
    "    churn_df.to_pickle(churn_file)\n",
    "    print(f\"Churn DataFrame saved to {churn_file}.\")\n",
    "\n",
    "# Prepare the final dataset for modeling\n",
    "print(\"Preparing final dataset for making the classification model...\")\n",
    "data = prepare_data(features_df, churn_df)\n",
    "print(\"\\nMerged DataFrame (Features + Churn Labels):\")\n",
    "print(data.head())\n",
    "\n",
    "# Save the prepared data\n",
    "prepared_data_file = 'prepared_data.pkl'\n",
    "data.to_pickle(prepared_data_file)\n",
    "print(f\"Prepared data saved to {prepared_data_file}.\")\n",
    "\n",
    "# Load prepared data if available\n",
    "prepared_data_file = 'prepared_data.pkl'\n",
    "if os.path.exists(prepared_data_file):\n",
    "    print(f\"Prepared data found at {prepared_data_file}. Loading it...\")\n",
    "    data = pd.read_pickle(prepared_data_file)\n",
    "else:\n",
    "    print(\"Prepared data not found. Please run the previous cell to prepare data.\")\n",
    "\n",
    "# Split the data into training and testing sets based on months\n",
    "print(\"Splitting data into training and testing sets...\")\n",
    "months = sorted(data['month'].unique())\n",
    "train_months = months[:-2]  # Use all months except the last two for training\n",
    "test_months = months[-2:]   # Use the last two months for testing\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_data = data[data['month'].isin(train_months)].reset_index(drop=True)\n",
    "test_data = data[data['month'].isin(test_months)].reset_index(drop=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "print(\"Separating features and target variable...\")\n",
    "X_train = train_data.drop(['user_id', 'month', 'churn'], axis=1)\n",
    "y_train = train_data['churn']\n",
    "X_test = test_data.drop(['user_id', 'month', 'churn'], axis=1)\n",
    "y_test = test_data['churn']\n",
    "\n",
    "# Train the model or load it if already trained\n",
    "model_file = 'churn_prediction_model.pkl'\n",
    "if os.path.exists(model_file):\n",
    "    print(f\"Model found at {model_file}. Loading it...\")\n",
    "    model = joblib.load(model_file)\n",
    "else:\n",
    "    print(\"Training the model...\")\n",
    "    model = train_model(X_train, y_train)\n",
    "    # Save the trained model for future use\n",
    "    joblib.dump(model, model_file)\n",
    "    print(f\"\\nModel training complete and saved as '{model_file}'.\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "print(\"Making predictions on the test set...\")\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Retained', 'Churned']))\n",
    "\n",
    "# Calculate ROC-AUC Score\n",
    "if hasattr(model.named_steps['classifier'], 'predict_proba'):\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "else:\n",
    "    print(\"ROC-AUC Score cannot be calculated for this classifier.\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "print(\"Plotting confusion matrix...\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=['Retained', 'Churned'],\n",
    "    yticklabels=['Retained', 'Churned']\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Display feature importance\n",
    "if hasattr(model.named_steps['classifier'], 'feature_importances_'):\n",
    "    print(\"Displaying feature importance...\")\n",
    "    importances = model.named_steps['classifier'].feature_importances_\n",
    "    feat_importances = pd.Series(importances, index=X_train.columns)\n",
    "    feat_importances = feat_importances.sort_values(ascending=False)\n",
    "\n",
    "    # Plot the top 10 important features\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    feat_importances.head(10).plot(kind='barh')\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Top 10 Important Features')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importance is not available for this classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
